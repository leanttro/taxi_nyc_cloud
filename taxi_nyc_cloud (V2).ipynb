{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81rlhA2uQGF8"
      },
      "source": [
        "- #  Configuração do Ambiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IhQ3r7F7MR5n"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark google-cloud-bigquery matplotlib seaborn -q\n",
        "!pip install pandas-gbq -q\n",
        "!pip install pandas-gbq -q\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import pandas_gbq\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install pyspark pandas-gbq db-dtypes -q\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbxre47i-UiY"
      },
      "source": [
        "- # Configurações do BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN8iFF30-VGX"
      },
      "outputs": [],
      "source": [
        "project_id = 'leanttro-projeto-taxi'\n",
        "tabela_destino_bq = 'dados_analise.dados_brutos'\n",
        "colunas_para_usar = [\n",
        "    'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
        "    'passenger_count', 'trip_distance', 'total_amount']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_TYM_JNU-qM"
      },
      "source": [
        "- ##  Verifica o Último Mês Carregado no BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgIulabxUjUm"
      },
      "outputs": [],
      "source": [
        "print(\"Verificando o último mês carregado no BigQuery...\")\n",
        "start_date = '2023-01-01' # Data de início padrão se a tabela não existir\n",
        "ultima_data_carregada_str = \"Nenhuma\"\n",
        "\n",
        "try:\n",
        "    sql_ultima_data = f\"SELECT MAX(tpep_pickup_datetime) as ultima_data FROM `{project_id}.{tabela_destino_bq}`\"\n",
        "    df_ultima_data = pandas_gbq.read_gbq(sql_ultima_data, project_id=project_id)\n",
        "\n",
        "    ultima_data_carregada = df_ultima_data['ultima_data'].iloc[0]\n",
        "\n",
        "    if pd.notna(ultima_data_carregada):\n",
        "        # Move a data de início para o primeiro dia do MÊS SEGUINTE\n",
        "        start_date = (ultima_data_carregada + pd.offsets.MonthBegin(1)).strftime('%Y-%m-%d')\n",
        "        ultima_data_carregada_str = ultima_data_carregada.strftime('%Y-%m-%d')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"AVISO: Não foi possível verificar a última data (tabela pode não existir ainda). Começando do início. Erro: {e}\")\n",
        "\n",
        "print(f\"Último dado encontrado: {ultima_data_carregada_str}. Iniciando busca a partir de: {start_date}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22oCXtBVPU_"
      },
      "source": [
        "- ## Gera o Intervalo de Datas Dinamicamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3hmE675VYPy"
      },
      "source": [
        "O fim é sempre o primeiro dia do mês atual, para garantir que só peguemos meses completos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP7CLFwUVOnQ"
      },
      "outputs": [],
      "source": [
        "end_date = pd.to_datetime('today').replace(day=1).strftime('%Y-%m-%d')\n",
        "datas_para_carregar = pd.date_range(start=start_date, end=end_date, freq='MS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX7pUGgdVIyv"
      },
      "source": [
        "- ## Loop de Carga Incremental: (ADICIONAR NOVOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6uG7md7VThw"
      },
      "outputs": [],
      "source": [
        "if datas_para_carregar.empty:\n",
        "    print(\"\\nNão há novos meses completos para carregar. Seus dados já estão atualizados!\")\n",
        "else:\n",
        "    print(f\"\\nIniciando o carregamento de {len(datas_para_carregar)} novo(s) mese(s) de dados...\\n\")\n",
        "\n",
        "    for data in datas_para_carregar:\n",
        "        ano = data.strftime('%Y')\n",
        "        mes = data.strftime('%m')\n",
        "\n",
        "        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{ano}-{mes}.parquet\"\n",
        "\n",
        "        print(f\"Processando: {ano}-{mes}\")\n",
        "        try:\n",
        "            response = requests.get(url, timeout=20)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            df_mensal = pd.read_parquet(BytesIO(response.content), columns=colunas_para_usar)\n",
        "            df_mensal = df_mensal.dropna(subset=['tpep_pickup_datetime'])\n",
        "\n",
        "            print(f\"Enviando {len(df_mensal):,} linhas para o BigQuery...\")\n",
        "\n",
        "            pandas_gbq.to_gbq(\n",
        "                df_mensal,\n",
        "                destination_table=tabela_destino_bq,\n",
        "                project_id=project_id,\n",
        "                if_exists='append'\n",
        "            )\n",
        "            print(f\"Dados de {ano}-{mes} carregados com sucesso!\\n\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"AVISO: Falha ao baixar dados de {ano}-{mes}. O arquivo pode não existir ainda ou houve um erro de rede. Erro: {e}\\n\")\n",
        "\n",
        "print(\"--- Carga de dados em lote concluída! ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y18iDf7MXsBa"
      },
      "source": [
        "- ## Puxando tabela silver (dados_limpos) do GCP:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-EKAWOXpaD"
      },
      "source": [
        "- # Configurações do BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IlsJqAzEH0Ii"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"PySparkAmostraGrande_5M\").getOrCreate()\n",
        "project_id = 'leanttro-projeto-taxi'\n",
        "table_id = 'dados_analise.dados_limpos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RslVbeCvadbG"
      },
      "source": [
        "- ## Movendo 2 milhões de registros pela rede do BigQuery para o Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcoMfqROaZu-",
        "outputId": "bc1d0b52-afeb-426b-85de-2b2741e1ca5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Buscando 2 milhões de registros do BigQuery para o Pandas...\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Amostra de 2,000,000 linhas carregada com sucesso no Pandas.\n",
            "\n",
            "Convertendo para DataFrame PySpark...\n"
          ]
        }
      ],
      "source": [
        "sql_query_amostra_grande = f\"SELECT * FROM `{project_id}.{table_id}` LIMIT 2000000\"\n",
        "print(f\"\\nBuscando 2 milhões de registros do BigQuery para o Pandas...\")\n",
        "df_pandas = pandas_gbq.read_gbq(sql_query_amostra_grande, project_id=project_id)\n",
        "print(f\"Amostra de {len(df_pandas):,} linhas carregada com sucesso no Pandas.\")\n",
        "print(\"\\nConvertendo para DataFrame PySpark...\")\n",
        "df_silver_spark = spark.createDataFrame(df_pandas)\n",
        "df_silver_spark.cache()\n",
        "print(\"DataFrame PySpark criado e pronto para uso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pceoWPVhRBE"
      },
      "source": [
        "- # Criando DataFrame Gold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QiMsFn3JeYVu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count, sum, avg, round, col\n",
        "df_gold_spark = df_silver_spark.groupBy(\"pickup_day_of_week\", \"pickup_hour\") \\\n",
        "                            .agg(\n",
        "                                count(\"*\").alias(\"total_de_corridas\"),\n",
        "                                round(sum(\"total_amount\"), 2).alias(\"faturamento_total\"),\n",
        "                                round(avg(\"total_amount\"), 2).alias(\"faturamento_medio_por_corrida\"),\n",
        "                                round(avg(\"trip_distance\"), 2).alias(\"distancia_media_percorrida\"),\n",
        "                                round(avg(\"trip_duration_minutes\"), 2).alias(\"duracao_media_minutos\"),\n",
        "                                round(avg(\"passenger_count\"), 2).alias(\"media_de_passageiros\"),\n",
        "                                round(avg(col(\"trip_distance\") / (col(\"trip_duration_minutes\") / 60)), 2).alias(\"velocidade_media_kmh\")\n",
        "                            )\\\n",
        "                            .filter(col(\"duracao_media_minutos\") > 0)\n",
        "\n",
        "df_gold_spark.cache()\n",
        "\n",
        "print(\"DataFrame Gold criado com sucesso!\")\n",
        "df_gold_spark.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF0MWve1hXjB"
      },
      "source": [
        "- # Análise Exploratória 2 milhões de Dados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCMRnR04iBSk"
      },
      "source": [
        "- TOTAL DE CORRIDAS POR HORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsO9Vun2hWJv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Calculando o total de corridas por hora...\")\n",
        "corridas_por_hora = df_gold_spark.groupBy(\"pickup_hour\") \\\n",
        "                               .agg(sum(\"total_de_corridas\").alias(\"volume_total_corridas\")) \\\n",
        "                               .orderBy(\"pickup_hour\")\n",
        "\n",
        "# Converter o resultado (que é pequeno, só 24 linhas) para Pandas para plotar\n",
        "corridas_por_hora_pd = corridas_por_hora.toPandas()\n",
        "\n",
        "# Gerar o gráfico\n",
        "print(\"Gerando o gráfico de barras...\")\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x='pickup_hour', y='volume_total_corridas', data=corridas_por_hora_pd, palette='YlOrRd')\n",
        "plt.title('Total de Corridas por Hora do Dia (Amostra de 2 Milhões)', fontsize=16)\n",
        "plt.xlabel('Hora do Dia', fontsize=12)\n",
        "plt.ylabel('Volume Total de Corridas', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COIBBGeriUDI"
      },
      "source": [
        "- FATURAMENTO POR DIA DA SEMANA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB9lJCxjiq7T"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Calculando faturamento por dia da semana...\")\n",
        "faturamento_por_dia = df_gold_spark.groupBy(\"pickup_day_of_week\") \\\n",
        "                                   .agg(sum(\"faturamento_total\").alias(\"faturamento\")) \\\n",
        "                                   .orderBy(\"pickup_day_of_week\")\n",
        "\n",
        "# Converter para Pandas para plotar\n",
        "faturamento_por_dia_pd = faturamento_por_dia.toPandas()\n",
        "\n",
        "# Mapear os números dos dias para nomes para o gráfico ficar mais legível\n",
        "dias_map = {1: 'Domingo', 2: 'Segunda', 3: 'Terça', 4: 'Quarta', 5: 'Quinta', 6: 'Sexta', 7: 'Sábado'}\n",
        "faturamento_por_dia_pd['dia_nome'] = faturamento_por_dia_pd['pickup_day_of_week'].map(dias_map)\n",
        "\n",
        "\n",
        "# Gerar o gráfico\n",
        "print(\"Gerando o gráfico de pizza...\")\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.pie(faturamento_por_dia_pd['faturamento'], labels=faturamento_por_dia_pd['dia_nome'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('YlOrRd', 7))\n",
        "plt.title('Distribuição do Faturamento por Dia da Semana', fontsize=16)\n",
        "plt.ylabel('') # Remove o label 'faturamento' do eixo y\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAyT-_-ei2if"
      },
      "source": [
        "- GRÁFICO COMBINADO DE TRÂNSITO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hoLy-wfi1xP"
      },
      "outputs": [],
      "source": [
        "print(\"Calculando métricas de trânsito por hora...\")\n",
        "transito_por_hora = df_gold_spark.groupBy(\"pickup_hour\") \\\n",
        "                               .agg(avg(\"duracao_media_minutos\").alias(\"duracao_media\"),\n",
        "                                    avg(\"velocidade_media_kmh\").alias(\"velocidade_media\")) \\\n",
        "                               .orderBy(\"pickup_hour\")\n",
        "\n",
        "transito_por_hora_pd = transito_por_hora.toPandas()\n",
        "print(\"Gerando o gráfico combinado...\")\n",
        "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "sns.barplot(x='pickup_hour', y='duracao_media', data=transito_por_hora_pd, color='gold', ax=ax1, label='Duração Média (min)')\n",
        "ax1.set_xlabel('Hora do Dia', fontsize=12)\n",
        "ax1.set_ylabel('Duração Média (minutos)', fontsize=12, color='gold')\n",
        "ax1.tick_params(axis='y', labelcolor='gold')\n",
        "\n",
        "# Criar um segundo eixo Y para a Velocidade\n",
        "ax2 = ax1.twinx()\n",
        "sns.lineplot(x='pickup_hour', y='velocidade_media', data=transito_por_hora_pd, color='red', marker='o', ax=ax2, label='Velocidade Média (km/h)')\n",
        "ax2.set_ylabel('Velocidade Média (km/h)', fontsize=12, color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "plt.title('Análise de Trânsito: Duração da Viagem vs. Velocidade Média', fontsize=16)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lql4aRMUEvOd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3pQNJm5Lgg"
      },
      "source": [
        "## **MACHINE LEARNING (AMOSTA 2 MILHÕES CORRIDAS):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUojeBj8_i17"
      },
      "source": [
        "#### Ensinar um modelo a prever a coluna trip_duration_minutes com base em outras características da corrida com PySpark MLlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0cBkxQtP5KVJ",
        "outputId": "0f227c5f-6edf-4114-c200-1cbaa8516452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usuário autenticado com o Google Cloud!\n",
            "\n",
            "Carregando 2 milhões de registros do BigQuery...\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o94.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1075872817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_silver_completo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bigquery'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{project_id}:dados_analise.dados_limpos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Aplicando o mesmo limite de 2M do seu notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o94.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from google.colab import auth\n",
        "spark = SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"Colab-PySpark-ML-Taxi\")\\\n",
        "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 2. Autenticar com sua conta Google\n",
        "auth.authenticate_user()\n",
        "print('Usuário autenticado com o Google Cloud!')\n",
        "\n",
        "print(\"\\nCarregando 2 milhões de registros do BigQuery...\")\n",
        "df_silver_completo = spark.read.format('bigquery') \\\n",
        "  .option('table', f'{project_id}:dados_analise.dados_limpos') \\\n",
        "  .load() \\\n",
        "  .limit(2000000) # Aplicando o mesmo limite de 2M do seu notebook\n",
        "\n",
        "# Colocar em cache para performance\n",
        "df_silver_completo.cache()\n",
        "\n",
        "print(f\"Dados carregados! Total de {df_silver_completo.count()} registros.\")\n",
        "df_silver_completo.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIG8lt8Q6C4o"
      },
      "source": [
        "- Carregando os Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZLo2YXNEvgL"
      },
      "outputs": [],
      "source": [
        "df_modelo = df_silver_spark.select(\n",
        "    \"pickup_hour\",\n",
        "    \"pickup_day_of_week\",\n",
        "    \"trip_distance\",\n",
        "    \"passenger_count\",\n",
        "    \"trip_duration_minutes\",\n",
        "    \"total_amount\"\n",
        ").na.drop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2-pHhO86Umm"
      },
      "source": [
        "- Dividindo os dados em treino (80%) e teste (20%):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP-uw2ai6bEW"
      },
      "outputs": [],
      "source": [
        "(df_treino, df_teste) = df_modelo.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNWyIPrf6epu"
      },
      "source": [
        "-  Colocando em cache novamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QL6QIhy6dr-"
      },
      "outputs": [],
      "source": [
        "df_treino.cache()\n",
        "df_teste.cache()\n",
        "print(f\"Dados prontos para o modelo. Treino: {df_treino.count()}, Teste: {df_teste.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJkINtty6oJW"
      },
      "source": [
        ".cache(): É uma otimização do Spark para guardar esses dois conjuntos de dados na memória, tornando o acesso a eles mais rápido nas etapas seguintes de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ua3Zkdg8HRS"
      },
      "source": [
        "## Preparando as Features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaiejQGu8JGi"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "features_cols = [\"pickup_hour\", \"pickup_day_of_week\", \"trip_distance\", \"passenger_count\"]\n",
        "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
        "\n",
        "# Transformando os dataframes de treino e teste:\n",
        "df_treino_ml = assembler.transform(df_treino)\n",
        "df_teste_ml = assembler.transform(df_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DY5i6177Sut"
      },
      "source": [
        "## **MODELO DE ML:** Regressão Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAyvx9vi8Ul6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKbJcg5L7h9z"
      },
      "source": [
        "-  Carregar dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUL-o57p6qIl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"trip_duration_minutes\")\n",
        "lr_modelo = lr.fit(df_treino_ml)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G86rZPti8hgB"
      },
      "source": [
        "- Fazendo previsões:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sogB_oqp8jO6"
      },
      "outputs": [],
      "source": [
        "previsoes_lr = lr_modelo.transform(df_teste_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5J6loJ_8kQ5"
      },
      "source": [
        "- Avaliando modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QteKQu6Q8pOB"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(labelCol=\"trip_duration_minutes\", predictionCol=\"prediction\")\n",
        "rmse_lr = evaluator.evaluate(previsoes_lr, {evaluator.metricName: \"rmse\"})\n",
        "r2_lr = evaluator.evaluate(previsoes_lr, {evaluator.metricName: \"r2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs-_XPxz8qJh"
      },
      "source": [
        "## - Resultado do Modelo Linear:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufEAALDn8t6B"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Resultado da Regressão Linear ---\")\n",
        "print(f\"RMSE: {rmse_lr:.2f} minutos\")\n",
        "print(f\"R²: {r2_lr:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9fB3FUO82YB"
      },
      "source": [
        "RMSE: Ele pode errar até 14.01 min pra mais ou pra menos do que a duração da viagem!\n",
        "\n",
        "Um R² de 0.35% não é negativo, como na amostra inferior, mas é considerado muito baixo. Ele está ativamente fazendo previsões ruins. POR ISSO, vamos treinar OUTRO modelo e avaliar novamente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtYOnKfN9F1A"
      },
      "source": [
        "## **MODELO DE ML:** Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78JdxhL99HnI"
      },
      "source": [
        "n_estimators=50 significa que ele usará 50 \"árvores de decisão\" para votar na melhor previsão.\n",
        "n_jobs=-1 usa todos os processadores disponíveis para acelerar o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE6DTUTv9UB_"
      },
      "source": [
        "- Carregando dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlYBiKKk9PlA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"trip_duration_minutes\",\n",
        "    seed=42,\n",
        "    numTrees=50  # Equivalente ao n_estimators\n",
        ")\n",
        "rf_modelo = rf.fit(df_treino_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msG_svP9WFH"
      },
      "source": [
        "- Fazendo previsões:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpZaqasu9X1A"
      },
      "outputs": [],
      "source": [
        "previsoes_rf = rf_modelo.transform(df_teste_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDU-9Plt9bp4"
      },
      "source": [
        "- Avaliando o modelo (usando o mesmo 'evaluator' de antes):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzbl23949h2v"
      },
      "outputs": [],
      "source": [
        "rmse_rf = evaluator.evaluate(previsoes_rf, {evaluator.metricName: \"rmse\"})\n",
        "r2_rf = evaluator.evaluate(previsoes_rf, {evaluator.metricName: \"r2\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kVlKO6K-jql"
      },
      "source": [
        "## - Resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqSQs1qM9lrf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Resultado do Random Forest ---\")\n",
        "print(f\"RMSE: {rmse_rf:.2f} minutos\")\n",
        "print(f\"R²: {r2_rf:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGZisBAd9jd3"
      },
      "source": [
        "%md\n",
        "RMSE: Ele pode errar até 7.59 min pra mais ou pra menos do que a duração da viagem!\n",
        "\n",
        "Um R² de 70.75% significa que ele pode acertar mais que 2/3 das previsões! Bem melhor do que o modelo de regressão linear para esse caso!\n",
        "\n",
        "**OU SEJA, OBTEVE UM RESULTADO MELHOR QUE O PRIMEIRO TESTE!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le4vCTJa_TEF"
      },
      "source": [
        "#### **Ensinar um modelo a prever a coluna total_amount com base nas características da corrida, incluindo a duração que nosso primeiro modelo previa usando PySpark MLlib**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4SxLNzAtbh"
      },
      "source": [
        "**MODELO DE ML:** Random Forest Regressor:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXVstaXXBUtn"
      },
      "source": [
        "- Criando coluna \"Duração Prevista\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k9fBptBBPFf"
      },
      "outputs": [],
      "source": [
        "df_treino_com_previsao_tempo = rf_modelo.transform(df_treino_ml) \\\n",
        "                                        .withColumnRenamed(\"prediction\", \"duracao_prevista\")\n",
        "df_teste_com_previsao_tempo = rf_modelo.transform(df_teste_ml) \\\n",
        "                                       .withColumnRenamed(\"prediction\", \"duracao_prevista\")\n",
        "\n",
        "print(\"Coluna 'duracao_prevista' criada com sucesso.\")\n",
        "df_treino_com_previsao_tempo.select(\"trip_duration_minutes\", \"duracao_prevista\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKtnRMVsBYqv"
      },
      "source": [
        "- Criando Features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJM6yfeFBH24"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "features_valor_cols = [\n",
        "    \"pickup_hour\",\n",
        "    \"pickup_day_of_week\",\n",
        "    \"trip_distance\",\n",
        "    \"passenger_count\",\n",
        "    \"duracao_prevista\"  # <<< A previsão do primeiro modelo entra aqui!\n",
        "]\n",
        "assembler_valor = VectorAssembler(inputCols=features_valor_cols, outputCol=\"features_valor\")\n",
        "df_treino_valor = assembler_valor.transform(df_treino_com_previsao_tempo)\n",
        "df_teste_valor = assembler_valor.transform(df_teste_com_previsao_tempo)\n",
        "print(\"Features para o modelo de valor preparadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG6C6zDHAuvp"
      },
      "source": [
        "- Carrendo dados focado no valor (total_amount):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gummK7QA0SB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "rf_valor = RandomForestRegressor(\n",
        "    featuresCol=\"features_valor\",\n",
        "    seed=42,\n",
        "    numTrees=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNUFmKiDA6t4"
      },
      "source": [
        "- Treinando modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMWslE18A9Wa"
      },
      "outputs": [],
      "source": [
        "# Treinar o modelo\n",
        "modelo_valor = rf_valor.fit(df_treino_valor)\n",
        "print(\"Treinamento concluído!\")\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "previsoes_valor = modelo_valor.transform(df_teste_valor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYxNrnrB6T2"
      },
      "source": [
        "- Avaliando o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMKU23BAB52n"
      },
      "outputs": [],
      "source": [
        "evaluator_valor = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\")\n",
        "rmse_valor = evaluator_valor.evaluate(previsoes_valor, {evaluator_valor.metricName: \"rmse\"})\n",
        "r2_valor = evaluator_valor.evaluate(previsoes_valor, {evaluator_valor.metricName: \"r2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of28SaPGCAxu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8UtY8UzCAVg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Resultado do Modelo de Previsão de Valor ---\")\n",
        "print(f\"RMSE: ${rmse_valor:.2f} DÓLARES\")\n",
        "print(f\"R²: {r2_valor:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmC0J3uSD5GT"
      },
      "source": [
        "RMSE: O valor está R$7,48 DÓLARES de diferença (pra mais ou pra menos) do valor real, considerando todas as variações que podem alterar no valor final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BdgKUSUD8Xh"
      },
      "source": [
        "R²: 86.64% significa que o modelo  consegue captar bem a relação entre as variáveis (tempo, distância, hora, etc.) e o preço."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CpLGlVnCYB9"
      },
      "source": [
        "# **SIMULADOR DE PREÇOS PARA CORRIDA (AMOSTRA 2 MILHÕES DE CORRIDAS):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXzYPziXCatQ"
      },
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# SIMULADOR COMPLETO (TEMPO E VALOR) COM MODELOS PYSPARK\n",
        "# =================================================================\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "\n",
        "# Função que usa os dois modelos PySpark salvos\n",
        "def simular_corrida_spark_completo(km, hora, dia_semana, passageiros=1):\n",
        "\n",
        "    # --- 1. Carregar os modelos treinados ---\n",
        "    try:\n",
        "        modelo_tempo = RandomForestRegressionModel.load(\"./modelo_tempo_spark\")\n",
        "        modelo_valor = RandomForestRegressionModel.load(\"./modelo_valor_spark\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar os modelos. Certifique-se de que as pastas './modelo_tempo_spark' e './modelo_valor_spark' existem.\")\n",
        "        print(f\"Detalhe do erro: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Preparar os dados de ENTRADA do usuário ---\n",
        "    milhas = km * 0.621371\n",
        "    schema_inicial = \"pickup_hour INT, pickup_day_of_week INT, trip_distance DOUBLE, passenger_count INT\"\n",
        "    dados_nova_corrida = spark.createDataFrame(\n",
        "        data=[(hora, dia_semana, milhas, passageiros)],\n",
        "        schema=schema_inicial\n",
        "    )\n",
        "\n",
        "    # --- 3. PREVISÃO DO TEMPO (PRIMEIRO MODELO) ---\n",
        "    # Transforma os dados de entrada usando o primeiro assembler (para o modelo de tempo)\n",
        "    dados_para_prever_tempo = assembler.transform(dados_nova_corrida)\n",
        "    # Faz a previsão do tempo e renomeia a coluna de previsão para 'duracao_prevista'\n",
        "    df_com_duracao_prevista = modelo_tempo.transform(dados_para_prever_tempo).withColumnRenamed(\"prediction\", \"duracao_prevista\")\n",
        "\n",
        "    # Extrai o valor da duração prevista\n",
        "    duracao_prevista = df_com_duracao_prevista.select(\"duracao_prevista\").first()[0]\n",
        "\n",
        "    # --- 4. PREVISÃO DO VALOR (SEGUNDO MODELO) ---\n",
        "    # Transforma o DataFrame (que agora tem a 'duracao_prevista') usando o segundo assembler (para o modelo de valor)\n",
        "    dados_para_prever_valor = assembler_valor.transform(df_com_duracao_prevista)\n",
        "    # Faz a previsão do valor\n",
        "    previsao_valor_df = modelo_valor.transform(dados_para_prever_valor)\n",
        "\n",
        "    # Extrai o resultado da previsão de valor\n",
        "    valor_previsto = previsao_valor_df.select(\"prediction\").first()[0]\n",
        "\n",
        "    # --- 5. Exibir o resultado (SAÍDA para o usuário) ---\n",
        "    print(\"\\n--- Previsão da Corrida (PySpark) ---\")\n",
        "    print(f\"Distância: {km:.2f} km\")\n",
        "    print(f\"Hora: {hora}:00h\")\n",
        "    print(f\"Dia da semana: {dia_semana}\")\n",
        "    print(f\">> Tempo estimado: {duracao_prevista:.1f} min\")\n",
        "    print(f\">> Valor estimado: ${valor_previsto:.2f}\")\n",
        "\n",
        "# --- Parte Interativa ---\n",
        "# (Pede os dados para o usuário e chama a função completa)\n",
        "try:\n",
        "    print(\"\\n--- Simulador de Corrida Interativo (PySpark) ---\")\n",
        "    km_usuario = float(input(\"Digite a distância da corrida em KM (ex: 12.5): \"))\n",
        "    hora_usuario = int(input(\"Digite a hora do dia (0 a 23): \"))\n",
        "    dia_semana_usuario = int(input(\"Digite o dia da semana (1=Dom, 2=Seg, ..., 7=Sáb): \"))\n",
        "\n",
        "    simular_corrida_spark_completo(km_usuario, hora_usuario, dia_semana_usuario)\n",
        "\n",
        "except ValueError:\n",
        "    print(\"\\nErro: Entrada inválida.\")\n",
        "except NameError:\n",
        "    print(\"\\nErro: Os 'assemblers' não foram definidos. Execute o código de treinamento primeiro.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}